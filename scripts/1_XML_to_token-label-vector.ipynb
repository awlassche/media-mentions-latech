{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import fasttext\n",
    "import fileinput\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "TOKENIZER = nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XML to token-label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser()\n",
    "punctuation = \"[{`,.?!:;/\\()''\"\"¬}]\"\n",
    "\n",
    "all_chronicles = glob.glob(os.path.join('PATH_TO_DIRECTORY_WITH_XML-FILES/*.xml'))\n",
    "\n",
    "for file in all_chronicles:\n",
    "    xml = etree.parse(file)\n",
    "\n",
    "    for elem in xml.getiterator():\n",
    "        elem.tag = etree.QName(elem).localname\n",
    "    etree.cleanup_namespaces(xml)\n",
    "    \n",
    "    previous_row = dict()\n",
    "    tokens = []\n",
    "\n",
    "    line_elements = xml.xpath('//l')\n",
    "    for i, line in tqdm(enumerate(line_elements),\n",
    "                       total=len(line_elements)):\n",
    "        filename = file[-22:-8]\n",
    "        for element in line.xpath('child::text()|*'):\n",
    "            if type(element) == etree._ElementUnicodeResult:\n",
    "                label = 'O'\n",
    "                attribute = ''\n",
    "                wordstring = re.sub(r\"((¬#?) ?)\", \"\", str(element))\n",
    "                for token in TOKENIZER(str(wordstring)):\n",
    "                    tokens.append(dict(sentence_id = i,\n",
    "                                    filename = filename,\n",
    "                                   token = token,\n",
    "                                   label = label,\n",
    "                                   attribute = attribute\n",
    "                                   ))\n",
    "                    previous_row = dict()\n",
    "            else:\n",
    "                if len(previous_row) == 0:\n",
    "                    label = element.xpath('name()') + '-B'\n",
    "                else:\n",
    "                    if previous_row['label'] == (element.xpath('name()') + '-I') or previous_row['label'] == (element.xpath('name()') + '-B'):\n",
    "                        label = element.xpath('name()') + '-I'\n",
    "                    else:\n",
    "                        label = element.xpath('name()') + '-B'\n",
    "                text = ''.join(element.xpath('descendant::text()'))\n",
    "                if label == 'hi-B':\n",
    "                    label = 'O'\n",
    "                if label == 'waarneming-B' or label == 'waarneming-I':\n",
    "                    attribute = ''.join(element.xpath('@waarneming'))\n",
    "                wordstring = str(text)\n",
    "                for j, token in (enumerate(TOKENIZER(str(wordstring)))):\n",
    "                    if j > 0 and label != '':\n",
    "                        label = element.xpath('name()') + '-I'\n",
    "                        if label == 'hi-I':\n",
    "                            label = 'O'\n",
    "                            attribute = ''\n",
    "                    tokens.append(dict(sentence_id = i,\n",
    "                                       filename = filename,\n",
    "                                       token = token,\n",
    "                                       label = label,\n",
    "                                       attribute = attribute\n",
    "                                       ))\n",
    "                    previous_row = dict(sentence_id = i,\n",
    "                                        filename = filename,\n",
    "                                       token = token,\n",
    "                                       label = label,\n",
    "                                       attribute = attribute\n",
    "                                       )\n",
    "    tokenized_text = pd.DataFrame(tokens)\n",
    "    tokenized_text.to_csv(file[:-7] + 'token-label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split into train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token_label_files = glob.glob(os.path.join('PATH_TO_DIRECTORY_WITH_TOKEN-LABEL-FILES/*label.csv'))\n",
    "\n",
    "for file in all_token_label_files:\n",
    "    df = pd.read_csv(file, sep=',', index_col=0)\n",
    "    df_train = df.iloc[:(len(df) - round(len(df)*0.3)), :]\n",
    "    df_test = df.iloc[len(df) - round(len(df)*0.3):, :]\n",
    "    df_train.to_csv(file[:-4] + '_train.csv', sep='\\t')\n",
    "    df_test.to_csv(file[:-4] + '_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Merge train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_files = glob.glob(os.path.join('PATH_TO_DIRECTORY_WITH_TRAIN-FILES/*train.csv'))\n",
    "\n",
    "file_list = []\n",
    "for file in all_train_files:\n",
    "    df = pd.read_csv(file, sep='\\t', index_col = 0)\n",
    "    file_list.append(df)\n",
    "\n",
    "total_train = pd.concat(file_list, ignore_index=True, sort=False).drop(['sentence_id'], 1)\n",
    "total_train.to_csv('all_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Merge test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_files = glob.glob(os.path.join('PATH_TO_DIRECTORY_WITH_TEST-FILES/*test.csv'))\n",
    "\n",
    "file_list = []\n",
    "for file in all_test_files:\n",
    "    df = pd.read_csv(file, sep='\\t', index_col = 0)\n",
    "    file_list.append(df)\n",
    "\n",
    "total_test = pd.concat(file_list, ignore_index=True, sort=False).drop(['sentence_id'], 1)\n",
    "total_test.to_csv('all_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train word embedding model with fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = xml.find('//title')\n",
    "fname = title.text\n",
    "text = xml.find('//text')\n",
    "chronicle = ''.join(text.itertext())\n",
    "wordstring = re.sub(r\"((¬#?) ?)\", \"\", chronicle.lower())\n",
    "for c in wordstring:\n",
    "    if c in punctuation:\n",
    "        wordstring = wordstring.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(fname) + '.txt', 'w') as f:\n",
    "    f.write(str(TOKENIZER(wordstring)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"PATH_TO_DIRECTORY_WITH_CORPUS_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding vectors to train and test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"PATH_TO_FASTTEXT_MODEL/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = pd.read_csv('all_train.csv', index_col=0)\n",
    "total_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test = pd.read_csv('all_test.csv', index_col=0)\n",
    "total_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Add vectors to train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "for index, row in tqdm(total_train.iterrows()):\n",
    "    row_dict = dict(row)\n",
    "    row_dict['vector'] = model.get_word_vector(row_dict['token'].lower())\n",
    "    all_rows.append(row_dict)\n",
    "\n",
    "total_train_vectors = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_vectors.to_csv('all_train_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Add vectors to test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "for index, row in tqdm(total_test.iterrows()):\n",
    "    row_dict = dict(row)\n",
    "    row_dict['vector'] = model.get_word_vector(row_dict['token'].lower())\n",
    "    all_rows.append(row_dict)\n",
    "\n",
    "total_test_vectors = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_vectors.to_csv('all_test_vectors.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
